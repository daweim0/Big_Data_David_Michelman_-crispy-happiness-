{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# Lab 5: Analysis of the K-Nearest-Neighbors Algorithm\n",
    "By David Michelman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import csv\n",
    "import copy\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains generally useful code for statistical analysis. It isn't specific to this data set in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bar_graph(data_array, index_x, index_y, title, y_title, headers):\n",
    "    \"\"\"\n",
    "    Shows a plot of the passed data, plt.show() must be called after all calls of bar_graph()\n",
    "    :param data_array: data array (must be 2 dimensional)\n",
    "    :param index_x: Coordinates for x axises (in form [x axis 1, x axis 2, x axis 3, ... x axis n]\n",
    "    :param index_y: Coordinates for Y axis\n",
    "    :param title: Title of graph\n",
    "    :param y_title: Y axis title\n",
    "    :param headers: the labels for the data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    fig1 = plt.figure()\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "\n",
    "    # create list of all unique values in x_index\n",
    "\n",
    "    x_values = find_unique_values(data_array, index_x[0])\n",
    "\n",
    "    # The following code is from the matplotlib api reference.\n",
    "    # It can be found here: http://matplotlib.org/examples/api/barchart_demo.html\n",
    "\n",
    "    averages = list()\n",
    "    for axis in index_x:\n",
    "        averages.append(filtered_average(data_array, index_y, [axis, ]))\n",
    "\n",
    "    ind = np.arange(len(index_x))  # the x locations for the groups\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects = list()\n",
    "    for i in range(len(averages)):\n",
    "        rects.append(ax.bar(ind, i, averages[i], color='r', yerr=1))\n",
    "\n",
    "    # add some text for labels, title and axes ticks\n",
    "\n",
    "    ax.set_xticks(ind + width)\n",
    "    ax.set_xticklabels(headers)\n",
    "\n",
    "    ax.legend(rects, ('Men', 'Women'))\n",
    "\n",
    "    def autolabel(rects):\n",
    "        # attach some text labels\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.text(rect.get_x() + rect.get_width() / 2., 1.05 * height,\n",
    "                    '%d' % int(height),\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "    autolabel(rects)\n",
    "\n",
    "    ax1.set_title(title + ')\\n')\n",
    "    ax1.set_ylabel(y_title)\n",
    "\n",
    "\n",
    "def find_unique_values(data, axis):\n",
    "    values = set()\n",
    "    for i in data[axis]:\n",
    "        values.update(i)\n",
    "    return list(values)\n",
    "\n",
    "\n",
    "def scatter_plot(data_array, index_x, index_y, title, x_title, y_title, compute_best_fit=False, square_axes=False, log_axes=None) -> None:\n",
    "    \"\"\"\n",
    "    Shows a plot of the passed data, plt.show() must be called after all calls of scatter_plot()\n",
    "    :param data_array: data array (must be 2 dimensional)\n",
    "    :param index_x: Coordinates for x axis or array with x data or array of x data\n",
    "    :param index_y: Coordinates for Y axis or array with y data or array of y data\n",
    "    :param title: Title of graph\n",
    "    :param x_title: X axis title\n",
    "    :param y_title: Y axis title\n",
    "    :param compute_best_fit: Should a best fit line be computed? (boolean)\n",
    "    :param square_axes: Forces axes to be square\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    fig1 = plt.figure()\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "    \n",
    "    if isinstance(index_x, collections.Iterable):\n",
    "        x_indices = index_x\n",
    "        y_indices = index_y\n",
    "    else:\n",
    "        x_indices = data_array[index_x]\n",
    "        y_indices = data_array[index_y]\n",
    "\n",
    "    if compute_best_fit:\n",
    "        sorted = list()\n",
    "        if not isinstance(index_x, collections.Iterable):\n",
    "            for i in range(data_array.shape[1]):\n",
    "                if not (math.isnan(data_array[index_x][i]) or math.isnan(data_array[index_y][i])):\n",
    "                    sorted.append([data_array[index_x][i], data_array[index_y][i]])\n",
    "        else:\n",
    "            for i in range(len(x_indices)):\n",
    "                sorted.append([x_indices[i], y_indices[i]])\n",
    "\n",
    "        # this line comes from Stephen on stack exchange\n",
    "        # (http://stackoverflow.com/questions/3121979/how-to-sort-list-tuple-of-lists-tuples)\n",
    "        sorted.sort(key=lambda tup: tup[1])\n",
    "\n",
    "        x = list()\n",
    "        y = list()\n",
    "        for i in sorted:\n",
    "            x.append(i[0])\n",
    "            y.append(i[1])\n",
    "\n",
    "        # the next 3 lines come from DSM on stack exchange\n",
    "        # (http://stackoverflow.com/questions/6148207/linear-regression-with-matplotlib-numpy)\n",
    "        fit = np.polyfit(x, y, 1)\n",
    "        fit_fn = np.poly1d(fit)\n",
    "        ax1.plot(x, y, 'yo', x, fit_fn(x), '--k')\n",
    "\n",
    "        if not isinstance(index_x, collections.Iterable):\n",
    "            ax1.set_title(title + '\\n (correlation coefecient = '\n",
    "                          + str(pearsonr_filtered(data_array[index_x], data_array[index_y])[0])\n",
    "                          + ', regression slope = ' + str(fit_fn[1]) + ')\\n')\n",
    "        else:\n",
    "            ax1.set_title(title + '\\n (correlation coefecient = '\n",
    "                          + str(stats.pearsonr(x_indices, y_indices)[0])\n",
    "                          + ', regression slope = ' + str(fit_fn[1]) + ')\\n')\n",
    "\n",
    "    else:\n",
    "        ax1.scatter(x_indices, y_indices)\n",
    "        ax1.set_title(title + '\\n')\n",
    "    if square_axes:\n",
    "        ax1.axis('equal')\n",
    "        ax1.axis([0, np.amax(x_indices) + 1, 0, np.amax(y_indices) + 1])\n",
    "    ax1.set_xlabel(x_title)\n",
    "    ax1.set_ylabel(y_title)\n",
    "    \n",
    "    if log_axes == \"x\" or log_axes == \"both\":\n",
    "        ax1.set_xscale('log')\n",
    "    if log_axes == \"y\" or log_axes == \"both\":\n",
    "        ax1.set_yscale('log')   \n",
    "\n",
    "\n",
    "def filtered_average_stdev(data, target, constraints, meets_constraint = False, contains=False, return_data=False):\n",
    "    \"\"\"\n",
    "    Calculates the average of columns in the passed numpy array data which meet some constraint\n",
    "    :param data: data array (must be 2 dimensional)\n",
    "    :param constraints: columns which must equal some value (in form [[0, 2013], [3, FAIL]], which means\n",
    "                        column 0 must equal 2013 and column 3 must equal FAIL).\n",
    "    :param target: column to be averaged\n",
    "    :param contains: if the constraint must only be contained in the line of data, not equal to the line of data\n",
    "    :return: average value\n",
    "    \"\"\"\n",
    "\n",
    "    scores = list()\n",
    "    for line in range(data.shape[1]):\n",
    "        failed = True\n",
    "        for rule in constraints:\n",
    "            if not contains and data[rule[0], line] == rule[1]:\n",
    "                failed = False\n",
    "            if contains and rule[1] in data[rule[0], line]:\n",
    "                failed = False\n",
    "        if failed is False:\n",
    "            try:\n",
    "                scores.append(float(data[target, line]))\n",
    "            except ValueError as e:\n",
    "                pass  # data[target, line] probably can't be turned into a float\n",
    "    try:\n",
    "        if not return_data:\n",
    "            return np.average(scores), np.std(scores)\n",
    "        else:\n",
    "            return scores\n",
    "    except ZeroDivisionError:\n",
    "        raise ZeroDivisionError(\n",
    "            'no float data fit constraints in filtered_average([data], ' + str(target) + ' ' + str(constraints))\n",
    "\n",
    "\n",
    "def print_correlation_array(data, headers):\n",
    "    \"\"\"\n",
    "    Prints a matrix of the correlations between all different variables\n",
    "    :param data: data array\n",
    "    :param headers: array of header values\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: make header_length changeable without messing up formatting\n",
    "\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "    # Make copy of headers then make them all the same length\n",
    "    headers_full = headers\n",
    "    headers = copy.deepcopy(headers)\n",
    "    header_length = 9\n",
    "    for i in range(len(headers)):\n",
    "        if len(headers[i]) > header_length:\n",
    "            headers[i] = headers[i][0:header_length]\n",
    "        while len(headers[i]) < header_length:\n",
    "            headers[i] += ' '\n",
    "    max_length = 0\n",
    "    for i in headers_full:\n",
    "        if len(i) > max_length:\n",
    "            max_length = len(i)\n",
    "    for i in range(len(headers_full)):\n",
    "        while len(headers_full[i]) < max_length:\n",
    "            headers_full[i] += ' '\n",
    "\n",
    "    # first z dimension is z values, second z dimension is p values\n",
    "    corelation_array = np.zeros([data.shape[0], data.shape[0]], dtype=np.float64)\n",
    "\n",
    "    for i in range(corelation_array.shape[0]):\n",
    "        for j in range(corelation_array.shape[1]):\n",
    "            if j < i:\n",
    "                corelation_array[i][j] = pearsonr_filtered(data[i], data[j])[0]\n",
    "            else:\n",
    "                corelation_array[i][j] = -2.0\n",
    "\n",
    "    coloring_bound_upper = 0.5\n",
    "    coloring_bound_lower = 0.3\n",
    "    for i in range(len(headers_full[0])):\n",
    "        print(' ', end='')\n",
    "    print(HEADER + \"      \" + str(headers[0:-1]).replace(\"', '\", ' ').replace(\"('\", '').replace(\"['\", '').replace(\"']\",\n",
    "                                                                                                                  '') + ENDC)\n",
    "    for i_num in range(1, len(corelation_array)):\n",
    "        if i_num < 10:\n",
    "            print(' ', end='')\n",
    "        print(HEADER + str(i_num) + ' ' + headers_full[i_num] + ENDC, end='')\n",
    "        i = corelation_array[i_num]\n",
    "        for j_num in range(len(i) - 1):\n",
    "            j = i[j_num]\n",
    "            if j_num > i_num - 1:\n",
    "                if j_num > i_num:\n",
    "                    print('    |     ', end='')\n",
    "                else:\n",
    "                    print('    \\u25BC     ', end='')  # the escape code is for a triangle pointing down\n",
    "            elif math.isnan(j):\n",
    "                print('   nan   ', end=' ')\n",
    "            elif j >= 0:\n",
    "                if 1.0 > abs(j) > coloring_bound_upper:\n",
    "                    print(OKGREEN + '   ' + '%.4f' % j + ENDC, end=' ')\n",
    "                elif 1.0 > abs(j) > coloring_bound_lower:\n",
    "                    print(OKBLUE + '   ' + '%.4f' % j + ENDC, end=' ')\n",
    "                else:\n",
    "                    print('   ' + '%.4f' % j, end=' ')\n",
    "            else:\n",
    "                if 1.0 > abs(j) > coloring_bound_upper:\n",
    "                    print(OKGREEN + '  ' + '%.4f' % j + ENDC, end=' ')\n",
    "                elif 1.0 > abs(j) > coloring_bound_lower:\n",
    "                    print(OKBLUE + '   ' + '%.4f' % j + ENDC, end=' ')\n",
    "                else:\n",
    "                    print('  ' + '%.4f' % j, end=' ')\n",
    "        print('   ' + HEADER + str(i_num) + ' ' + headers_full[i_num] + ENDC)\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def pearsonr_filtered(x, y):\n",
    "    # remove any nan values from x and y\n",
    "    x = list(copy.deepcopy(x))\n",
    "    y = list(copy.deepcopy(y))\n",
    "\n",
    "    initial_len = len(x)\n",
    "\n",
    "    # a while loop is used instead of a for loop because the list of x changes during loop execution\n",
    "    i = 0\n",
    "    while i < len(x):\n",
    "        try:\n",
    "            if math.isnan(float(x[i])) or math.isnan(float(y[i])):\n",
    "                x.pop(i)\n",
    "                y.pop(i)\n",
    "                i -= 1\n",
    "        except ValueError as e:\n",
    "            x.pop(i)\n",
    "            y.pop(i)\n",
    "            i -= 1\n",
    "        except TypeError as e:\n",
    "            x.pop(i)\n",
    "            y.pop(i)\n",
    "            i -= 1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    if len(x) < initial_len / 4.0:\n",
    "        # There was not enough numerical data to do proper analysis on\n",
    "        return math.nan, math.nan\n",
    "    return stats.pearsonr(np.array(x).astype(np.float), np.array(y).astype(np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "#set the random number generator seed so that the same values go into the test set every time\n",
    "random.seed(\"I'm a seeasdfd\")\n",
    "\n",
    "def euclidian_distance(item1, item2, dimensions):\n",
    "    distance = 0\n",
    "    for x in range(dimensions):\n",
    "        distance += math.pow(item1[x] - item2[x], 2)\n",
    "    return math.pow(distance, 0.5)\n",
    "\n",
    "\n",
    "def getNeighbors(trainingSet, testpoint, k, answers=None):\n",
    "    testpoint_hashable = str(testpoint)\n",
    "    if answers is not None:\n",
    "        if testpoint_hashable not in answers:\n",
    "            answers[testpoint_hashable] = getAllNeighbors(trainingSet, testpoint, answers=answers)\n",
    "        return answers[testpoint_hashable][0:k]\n",
    "    else:\n",
    "        return getAllNeighbors(trainingSet, testpoint, answers=answers)[0:k]\n",
    "\n",
    "\n",
    "def getAllNeighbors(trainingSet, testpoint, answers=None):\n",
    "    distances = []\n",
    "    for i in trainingSet:\n",
    "        distances.append((euclidian_distance(testpoint, i, len(i) - 1), i))\n",
    "    distances.sort(key=lambda tup: tup[0])\n",
    "    return distances\n",
    "\n",
    "\n",
    "def vote(neighbors):\n",
    "    labels = []\n",
    "    for i in neighbors:\n",
    "        labels.append(i[1][-1])\n",
    "    return Counter(labels).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def loadDataset(fine_name, split, trainingSet, testSet, columns_to_include = [0, 1]):\n",
    "    with open(fine_name, 'r') as csvfile:\n",
    "        lines = csv.reader(csvfile)\n",
    "        dataset = list(lines)\n",
    "        headers.append(dataset.pop(0))\n",
    "        n_attributes = len(dataset[0]) - 1\n",
    "        for x in range(len(dataset)-1):\n",
    "            for y in range(n_attributes):\n",
    "                if y in columns_to_include or y == 7:\n",
    "                    try:\n",
    "                        dataset[x][y] = float(dataset[x][y])\n",
    "                    except ValueError:\n",
    "                        date_components = dataset[x][y].replace(\"-\",\"\").replace(\":\",\" \").split(\" \")\n",
    "                        for i in range(len(date_components)):\n",
    "                            dataset[x][y] = int(date_components[i]) * 10**(len(date_components) - i)\n",
    "                else:\n",
    "                    dataset[x][y] = 0.0\n",
    "                    # print(\"excluded data with y=\", y)\n",
    "            if random.random() < split:\n",
    "                trainingSet.append(dataset[x])\n",
    "            else:\n",
    "                testSet.append(dataset[x])\n",
    "\n",
    "\n",
    "def get_error_rate(testSet, predictions):\n",
    "    correct = 0\n",
    "    for x in range(len(testSet)):\n",
    "        if testSet[x][-1] == predictions[x]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet)))*100.0\n",
    "\n",
    "\n",
    "def get_accuracy(k = 11, args_to_exclude=[1, 2, 3, 4, 5, 6, 7, 8, 9, 0]):\n",
    "    predictions = []\n",
    "    for i in testSet:\n",
    "        predictions.append(vote(getNeighbors(trainingSet, i, k, answers=answers)))\n",
    "    #print(predictions[len(predictions) - 1])\n",
    "    return(get_error_rate(testSet, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "k values vs accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, ended on k = 8318\n"
     ]
    }
   ],
   "source": [
    "headers = []\n",
    "trainingSet = []\n",
    "testSet = []\n",
    "answers = {}\n",
    "split = 0.9\n",
    "loadDataset('datatraining.txt', split, trainingSet, testSet)\n",
    "\n",
    "part_1_data = []\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "n_iterations = 1\n",
    "for k_val_raw in range(1, 3000, 1):\n",
    "    k_val = math.floor(1.05 ** k_val_raw)\n",
    "    if k_val > 8001:\n",
    "        break\n",
    "    if len(data_x) == 0 or data_x[len(data_x)-1] != k_val:\n",
    "        total = 0.0\n",
    "        start_time = time.time()\n",
    "        for i in range(n_iterations):\n",
    "            partial_sum = get_accuracy(k=k_val)\n",
    "            total += partial_sum\n",
    "        # print('    average accuracy with k=' + str(k_val) + \" is \" + str(total/n_iterations) + \"%\")\n",
    "        data_x.append(k_val)\n",
    "        data_y.append(total/n_iterations)\n",
    "part_1_data = (data_x, data_y)\n",
    "print(\"done, ended on k =\", k_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "which variables effect accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "collumns = list(np.linspace(1, 6, 6))\n",
    "\n",
    "for length in range(1, 6):\n",
    "    for included in itertools.combinations(collumns, length):\n",
    "        print(\"trying arguments \" + str(included), end='')\n",
    "        k = 11\n",
    "        trainingSet = []\n",
    "        testSet = []\n",
    "        answers = {}\n",
    "        split = 0.90\n",
    "        loadDataset('datatraining.txt', split, trainingSet, testSet)\n",
    "\n",
    "        running_total = 0\n",
    "        n_iterations = 1\n",
    "        for j in range(n_iterations):\n",
    "            running_total += get_accuracy(k=k, args_to_exclude = included)\n",
    "        average = running_total / n_iterations\n",
    "        print(\", resulted in average\", average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Part 2 raw data\n",
    "# 1 = date\t2 = Temperature\t3 = Humidity\t4 = Light\t5 = CO2\t6 = HumidityRatio\n",
    "part_2_data = [[1,0,0,0,0,0,99.26387778],[0,2,0,0,0,0,98.98226993],[0,0,3,0,0,0,98.95457921],[0,0,0,4,0,0,99.25966987],\n",
    "[0,0,0,0,5,0,98.76567642],[0,0,0,0,0,6,98.90746115],[1,2,0,0,0,0,99.04522211],[1,0,3,0,0,0,98.97997625],\n",
    "[1,0,0,4,0,0,98.82891971],[1,0,0,0,5,0,98.89071699],[1,0,0,0,0,6,98.98514016],[0,2,3,0,0,0,99.12929114],\n",
    "[0,2,0,4,0,0,98.91558073],[0,2,0,0,5,0,99.12986474],[0,2,0,0,0,6,98.91207529],[0,0,3,4,0,0,99.27363471],\n",
    "[0,0,3,0,5,0,99.01315256],[0,0,3,0,0,6,98.84000621],[0,0,0,4,5,0,98.95248014],[0,0,0,4,0,6,99.24932147],\n",
    "[0,0,0,0,5,6,98.75240225],[1,2,3,0,0,0,98.73042331],[1,2,0,4,0,0,99.0440702],[1,2,0,0,5,0,98.87418834],\n",
    "[1,2,0,0,0,6,99.38048194],[1,0,3,4,0,0,99.05137542],[1,0,3,0,5,0,98.87217169],[1,0,3,0,0,6,98.79964105],\n",
    "[1,0,0,4,5,0,98.87102899],[1,0,0,4,0,6,98.96199602],[1,0,0,0,5,6,99.37055744],[0,2,3,4,0,0,98.89449454],\n",
    "[0,2,3,0,5,0,98.9036136],[0,2,3,0,0,6,99.26095462],[0,2,0,4,5,0,99.21620236],[0,2,0,4,0,6,99.11742902],\n",
    "[0,2,0,0,5,6,99.12809066],[0,0,3,4,5,0,99.06646003],[0,0,3,4,0,6,99.19861429],[0,0,3,0,5,6,98.73437155],\n",
    "[0,0,0,4,5,6,99.11619875],[1,2,3,4,0,0,99.06177963],[1,2,3,0,5,0,99.04858554],[1,2,3,0,0,6,99.41634531],\n",
    "[1,2,0,4,5,0,98.91130171],[1,2,0,4,0,6,98.69824486],[1,2,0,0,5,6,99.25162725],[1,3,0,4,5,0,98.92288375],\n",
    "[1,3,0,4,0,6,99.08652054],[1,0,3,0,5,6,98.84687725],[1,0,0,4,5,6,99.09450132],[0,2,3,4,5,0,98.8746327],\n",
    "[0,2,3,4,0,6,99.30527739],[0,2,3,0,5,6,99.12279738],[0,2,0,4,5,6,99.1498565],[0,0,3,4,5,6,98.98613877],\n",
    "[1,2,3,4,5,0,99.1187591],[1,2,3,4,0,6,99.00238015],[1,2,3,0,5,6,99.24058323],[1,2,0,4,5,6,99.245664],\n",
    "[1,0,3,4,5,6,99.35240549],[0,2,3,4,5,6,99.05538287]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Analasys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scatter_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7e664edc056b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscatter_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpart_1_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Number of Neighbors Vs Accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Number of Neighbors\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mscatter_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpart_1_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Number of Neighbors Vs Accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Number of Neighbors\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_axes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scatter_plot' is not defined"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "scatter_plot(np.array(part_1_data), 0, 1, \"Number of Neighbors Vs Accuracy\", \"Number of Neighbors\", \"Accuracy\")\n",
    "scatter_plot(np.array(part_1_data), 0, 1, \"Number of Neighbors Vs Accuracy\", \"Number of Neighbors\", \"Accuracy\", log_axes='x')\n",
    "\n",
    "part_1_data_rearanged = list()\n",
    "for i in range(len(part_1_data[0])):\n",
    "    part_1_data_rearanged.append([part_1_data[0][i], part_1_data[1][i]])\n",
    "\n",
    "part_1_data_rearanged.sort(key=lambda x: -1 * x[1])\n",
    "print(\"k value, accuracy\")\n",
    "for i in range(0, 15):\n",
    "    print(str(part_1_data_rearanged[i][0]) + \",\", part_1_data_rearanged[i][1])\n",
    "\n",
    "even_sum = 0\n",
    "even_n = 0\n",
    "odd_sum = 0\n",
    "odd_n = 0\n",
    "for i in range(101):\n",
    "    if part_1_data_rearanged[i][0] % 2 == 0:\n",
    "        even_sum += part_1_data_rearanged[i][1]\n",
    "        even_n += 1\n",
    "    else:\n",
    "        odd_sum += part_1_data_rearanged[i][1]\n",
    "        odd_n += 1\n",
    "\n",
    "print()\n",
    "print(\"odd average accuracy =\", odd_sum / odd_n, \"(average where k ∈ [1,100])\")\n",
    "print(\"even average accuracy =\", even_sum / even_n, \"(average where k ∈ [1,100])\")\n",
    "print()\n",
    "maximum = np.amax(part_1_data[1])\n",
    "maximum_index = part_1_data[1].index(maximum)\n",
    "print(\"Highest accuracy was\", maximum, \"at k =\", part_1_data[0][maximum_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It would appear that lower numbers of nearest neighbors (k values) work the best. This result is slightly odd because k=3 and k=9 had higher accuracies than k=9. However, the difference between k=3 and k=5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Analasys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \u001b[95m      date      Temp      Humidity  Light     CO2       Humid%   \u001b[0m\n",
      " \u001b[95m1 Temp    \u001b[0m   0.0619     ▼         |         |         |         |        \u001b[95m1 Temp    \u001b[0m\n",
      " \u001b[95m2 Humidity\u001b[0m  -0.0970   -0.0560     ▼         |         |         |        \u001b[95m2 Humidity\u001b[0m\n",
      " \u001b[95m3 Light   \u001b[0m  -0.0323    0.0619   -0.0970     ▼         |         |        \u001b[95m3 Light   \u001b[0m\n",
      " \u001b[95m4 CO2     \u001b[0m  -0.0323   -0.0309   -0.0323   -0.0323     ▼         |        \u001b[95m4 CO2     \u001b[0m\n",
      " \u001b[95m5 Humid%  \u001b[0m  -0.0323   -0.0309   -0.0323   -0.0323   -0.0323     ▼        \u001b[95m5 Humid%  \u001b[0m\n",
      " \u001b[95m6 Accuracy\u001b[0m   0.0073    0.1489    0.0008    0.1223   -0.0596    0.2467    \u001b[95m6 Accuracy\u001b[0m\n",
      "\n",
      "\n",
      "attribute pairings and accuracies\n",
      "(1 = date, 2 = Temperature, 3 = Humidity, 4 = Light, 5 = CO2, 6 = HumidityRatio)\n",
      "1 2 3     6 99.41634531 \n",
      "1 2       6 99.38048194 \n",
      "1       5 6 99.37055744 \n",
      "1   3 4 5 6 99.35240549 \n",
      "  2 3 4   6 99.30527739 \n",
      "    3 4     99.27363471 \n",
      "1           99.26387778 \n",
      "  2 3     6 99.26095462 \n",
      "      4     99.25966987 \n",
      "1 2     5 6 99.25162725 \n",
      "      4   6 99.24932147 \n",
      "1 2   4 5 6 99.245664 \n",
      "1 2 3   5 6 99.24058323 \n",
      "  2   4 5   99.21620236 \n",
      "    3 4   6 99.19861429 \n",
      "  2   4 5 6 99.1498565 \n",
      "  2     5   99.12986474 \n",
      "  2 3       99.12929114 \n",
      "  2     5 6 99.12809066 \n"
     ]
    }
   ],
   "source": [
    "part_2_data_cp = copy.deepcopy(part_2_data)\n",
    "data_rawish = np.array(part_2_data)\n",
    "data = np.zeros([data_rawish.shape[1], data_rawish.shape[0]])\n",
    "\n",
    "for i in range(len(data_rawish)):\n",
    "    for j in range(len(data_rawish[0])):\n",
    "        data[j][i] = data_rawish[i][j]\n",
    "        \n",
    "print_correlation_array(data, [\"date\",\"Temp\",\"Humidity\",\"Light\",\"CO2\",\"Humid%\", \"Accuracy\"])\n",
    "\n",
    "part_2_data_cp.sort(key=lambda tup: tup[6])\n",
    "\n",
    "print(\"attribute pairings and accuracies\")\n",
    "print(\"(1 = date, 2 = Temperature, 3 = Humidity, 4 = Light, 5 = CO2, 6 = HumidityRatio)\")\n",
    "for i in range(1, 20):\n",
    "    for dat in part_2_data_cp[-1*i]:\n",
    "        if dat != 0:\n",
    "            print(dat, end=' ')\n",
    "        else:\n",
    "            print(\" \", end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear that Humidity Ratio and the date and time are the best indicators of if a room is occupied or not. \n",
    "\n",
    "Humanity Ratio was most correlated with occupancy, although the correlation coefficient was 0.24 which is on the borderline of being statistically significant. It was also present in the five most accurate combinations of indicators for use in the k-means algorithm. \n",
    "\n",
    "Date also appears to be one of the more useful features of the data for use in k-means, as it was in the four most accurate pairings of features. However, it has a correlation coefficient of 0.0073 with occupancy, which would imply that it is not a useful attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, it would appear that the best k values are in the 1 to 6 range. This largely agrees with common knowledge that the best k values are less than about 21. However, it also differs from common knowledge because using even values for k did not seem to yield significantly less accurate results than odd k values. \n",
    "\n",
    "Odd values of k are generally known to be more accurate than even values because taking the mode of an even number of values can result in a tie. However, according to the data collected, using k=n where n is an odd number is not always better than using k=n+1. This can be seen in k=4 resulting in 99.485% accuracy while k=3 resulted in 99.357% accuracy. In fact, the average accuracy of even k values was slightly higher than the average accuracy of odd k values from k=1 to k=100 (even average accuracy was 96.963 while the odd average accuracy was 96.242). While these differences are small, and their statistical significance has yet to be shown, they are still present and seem contradict common knowledge.\n",
    "\n",
    "One interesting effect that showed up in this data was that the time when a measurement was taken was a surprisingly good indicator of if a room is occupied or not. I suspect that this is because data normally appears in clusters in the training data set. IE: there are normally multiple consecutive measurements when a room is occupied. Therefore, the closest neighbor to any data point when only time is used will probably be for the same occupancy as the data point in question. This means that using time is a surprisingly useful attribute when using data that comes out of the training set. \n",
    "\n",
    "However, this probably won’t work as well with “real” data because the training set wouldn’t have any data taken from rooms where and while the algorithm was being employed. IE: the training set wouldn’t know if somebody was in the room directly before a “real” measurement was taken. \n",
    "\n",
    "A good way to test this would be to partition the first n% of the data set (by time) as training data then use the rest as testing data instead of randomly selecting points to use as training or testing data. This would do a better job of emulating real life because all of the training data would have been recorded chronologically before the testing data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
